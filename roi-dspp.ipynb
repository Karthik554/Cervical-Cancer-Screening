{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6243,"databundleVersionId":868544,"sourceType":"competition"}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-24T03:27:39.957416Z","iopub.execute_input":"2024-12-24T03:27:39.957820Z","iopub.status.idle":"2024-12-24T03:27:39.962050Z","shell.execute_reply.started":"2024-12-24T03:27:39.957789Z","shell.execute_reply":"2024-12-24T03:27:39.960989Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# import required libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport glob\nimport plotly.graph_objects as go\nimport cv2\nfrom PIL import Image\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n\nfrom PIL import ImageFile\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport tensorflow as tf\n#from keras.preprocessing.image import ImageDataGenerator\n#from keras.utils.np_utils import to_categorical\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n#from keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten , Dropout\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau,ModelCheckpoint \nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nImageFile.LOAD_TRUNCATED_IMAGES = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T03:27:39.963472Z","iopub.execute_input":"2024-12-24T03:27:39.963878Z","iopub.status.idle":"2024-12-24T03:27:52.360047Z","shell.execute_reply.started":"2024-12-24T03:27:39.963841Z","shell.execute_reply":"2024-12-24T03:27:52.359116Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"pip install yolov5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T03:28:00.680353Z","iopub.execute_input":"2024-12-24T03:28:00.680729Z","iopub.status.idle":"2024-12-24T03:28:13.307634Z","shell.execute_reply.started":"2024-12-24T03:28:00.680673Z","shell.execute_reply":"2024-12-24T03:28:13.306374Z"}},"outputs":[{"name":"stdout","text":"Collecting yolov5\n  Downloading yolov5-7.0.14-py37.py38.py39.py310-none-any.whl.metadata (10 kB)\nRequirement already satisfied: gitpython>=3.1.30 in /usr/local/lib/python3.10/dist-packages (from yolov5) (3.1.43)\nRequirement already satisfied: matplotlib>=3.3 in /usr/local/lib/python3.10/dist-packages (from yolov5) (3.7.1)\nRequirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from yolov5) (1.26.4)\nRequirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from yolov5) (4.10.0.84)\nRequirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from yolov5) (10.4.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from yolov5) (5.9.5)\nRequirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from yolov5) (6.0.2)\nRequirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from yolov5) (2.32.3)\nRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from yolov5) (1.13.1)\nCollecting thop>=0.1.1 (from yolov5)\n  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from yolov5) (2.4.1+cu121)\nRequirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from yolov5) (0.19.1+cu121)\nRequirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from yolov5) (4.66.5)\nCollecting ultralytics>=8.0.100 (from yolov5)\n  Downloading ultralytics-8.3.53-py3-none-any.whl.metadata (35 kB)\nRequirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from yolov5) (2.17.0)\nRequirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from yolov5) (2.1.4)\nRequirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from yolov5) (0.12.2)\nRequirement already satisfied: setuptools>=65.5.1 in /usr/local/lib/python3.10/dist-packages (from yolov5) (71.0.4)\nCollecting fire (from yolov5)\n  Downloading fire-0.7.0.tar.gz (87 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: boto3>=1.19.1 in /usr/local/lib/python3.10/dist-packages (from yolov5) (1.35.83)\nCollecting sahi>=0.11.10 (from yolov5)\n  Downloading sahi-0.11.20-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: huggingface-hub<0.25.0,>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from yolov5) (0.24.7)\nCollecting roboflow>=0.2.29 (from yolov5)\n  Downloading roboflow-1.1.50-py3-none-any.whl.metadata (9.7 kB)\nRequirement already satisfied: botocore<1.36.0,>=1.35.83 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.19.1->yolov5) (1.35.83)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.19.1->yolov5) (1.0.1)\nRequirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.19.1->yolov5) (0.10.4)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython>=3.1.30->yolov5) (4.0.11)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.25.0,>=0.12.0->yolov5) (3.16.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.25.0,>=0.12.0->yolov5) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.25.0,>=0.12.0->yolov5) (24.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.25.0,>=0.12.0->yolov5) (4.12.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->yolov5) (1.3.0)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->yolov5) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->yolov5) (4.53.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->yolov5) (1.4.7)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->yolov5) (3.1.4)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->yolov5) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->yolov5) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->yolov5) (2024.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->yolov5) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->yolov5) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->yolov5) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->yolov5) (2024.8.30)\nCollecting idna<4,>=2.5 (from requests>=2.23.0->yolov5)\n  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\nRequirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.10/dist-packages (from roboflow>=0.2.29->yolov5) (4.10.0.84)\nCollecting python-dotenv (from roboflow>=0.2.29->yolov5)\n  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow>=0.2.29->yolov5) (1.16.0)\nCollecting requests-toolbelt (from roboflow>=0.2.29->yolov5)\n  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\nCollecting filetype (from roboflow>=0.2.29->yolov5)\n  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: shapely>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sahi>=0.11.10->yolov5) (2.0.6)\nCollecting pybboxes==0.1.6 (from sahi>=0.11.10->yolov5)\n  Downloading pybboxes-0.1.6-py3-none-any.whl.metadata (9.9 kB)\nCollecting terminaltables (from sahi>=0.11.10->yolov5)\n  Downloading terminaltables-3.1.10-py2.py3-none-any.whl.metadata (3.5 kB)\nRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sahi>=0.11.10->yolov5) (8.1.7)\nRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->yolov5) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->yolov5) (1.64.1)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->yolov5) (3.7)\nRequirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->yolov5) (3.20.3)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->yolov5) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->yolov5) (3.0.4)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->yolov5) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->yolov5) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->yolov5) (3.1.4)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics>=8.0.100->yolov5) (9.0.0)\nCollecting ultralytics-thop>=2.0.0 (from ultralytics>=8.0.100->yolov5)\n  Downloading ultralytics_thop-2.0.13-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->yolov5) (2.4.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.30->yolov5) (5.0.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.4.1->yolov5) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.0->yolov5) (1.3.0)\nDownloading yolov5-7.0.14-py37.py38.py39.py310-none-any.whl (953 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.5/953.5 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading roboflow-1.1.50-py3-none-any.whl (81 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sahi-0.11.20-py3-none-any.whl (112 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pybboxes-0.1.6-py3-none-any.whl (24 kB)\nDownloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\nDownloading ultralytics-8.3.53-py3-none-any.whl (902 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m902.2/902.2 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ultralytics_thop-2.0.13-py3-none-any.whl (26 kB)\nDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\nDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\nDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\nBuilding wheels for collected packages: fire\n  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114250 sha256=23a32925482f22489a6ae831005754a2793ff03c21473e50774082b1740c769d\n  Stored in directory: /root/.cache/pip/wheels/19/39/2f/2d3cadc408a8804103f1c34ddd4b9f6a93497b11fa96fe738e\nSuccessfully built fire\nInstalling collected packages: filetype, terminaltables, python-dotenv, pybboxes, idna, fire, ultralytics-thop, thop, sahi, requests-toolbelt, ultralytics, roboflow, yolov5\n  Attempting uninstall: idna\n    Found existing installation: idna 3.10\n    Uninstalling idna-3.10:\n      Successfully uninstalled idna-3.10\nSuccessfully installed filetype-1.2.0 fire-0.7.0 idna-3.7 pybboxes-0.1.6 python-dotenv-1.0.1 requests-toolbelt-1.0.0 roboflow-1.1.50 sahi-0.11.20 terminaltables-3.1.10 thop-0.1.1.post2209072238 ultralytics-8.3.53 ultralytics-thop-2.0.13 yolov5-7.0.14\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import (Conv2D, BatchNormalization, Activation, \n                                     AveragePooling2D, UpSampling2D, concatenate, Dense, Flatten, Dropout)\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, precision_recall_fscore_support\nfrom skimage import measure\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.utils import shuffle\n\nimport cv2\nimport math\nimport os\nimport glob\nimport yolov5  # Assuming YOLOv5 is installed for object detection\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T03:28:17.378750Z","iopub.execute_input":"2024-12-24T03:28:17.379105Z","iopub.status.idle":"2024-12-24T03:49:22.016539Z","shell.execute_reply.started":"2024-12-24T03:28:17.379066Z","shell.execute_reply":"2024-12-24T03:49:22.015070Z"}},"outputs":[{"name":"stdout","text":"Creating new Ultralytics Settings v0.0.6 file ✅ \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n3\n6569 821 822\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-43bcebdcdcb2>\u001b[0m in \u001b[0;36m<cell line: 226>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDSPP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'DSPP' is not defined"],"ename":"NameError","evalue":"name 'DSPP' is not defined","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"def maxHist(hist):\n    maxArea = (0, 0, 0)\n    height = []\n    position = []\n    for i in range(len(hist)):\n        if not height:\n            if hist[i] > 0:\n                height.append(hist[i])\n                position.append(i)\n        else:\n            if hist[i] > height[-1]:\n                height.append(hist[i])\n                position.append(i)\n            elif hist[i] < height[-1]:\n                while height and height[-1] > hist[i]:\n                    maxHeight = height.pop()\n                    area = maxHeight * (i - position[-1])\n                    if area > maxArea[0]:\n                        maxArea = (area, position[-1], i)\n                    last_position = position.pop()\n                    if not height:\n                        break\n                position.append(last_position)\n                height.append(hist[i])\n    while height:\n        maxHeight = height.pop()\n        last_position = position.pop()\n        area = maxHeight * (len(hist) - last_position)\n        if area > maxArea[0]:\n            maxArea = (area, len(hist), last_position)\n    return maxArea\n\ndef maxRect(img):\n    maxArea = (0, 0, 0)\n    addMat = np.zeros(img.shape)\n    for r in range(img.shape[0]):\n        if r == 0:\n            addMat[r] = img[r]\n            area = maxHist(addMat[r])\n            if area[0] > maxArea[0]:\n                maxArea = area + (r,)\n        else:\n            addMat[r] = img[r] + addMat[r - 1]\n            addMat[r][img[r] == 0] = 0\n            area = maxHist(addMat[r])\n            if area[0] > maxArea[0]:\n                maxArea = area + (r,)\n    return (int(maxArea[3] + 1 - maxArea[0] / abs(maxArea[1] - maxArea[2])), maxArea[2], maxArea[3], maxArea[1], maxArea[0])\ndef cropCircle(img):\n    \"\"\"This function is replaced with YOLO detection for precise bounding box cropping.\"\"\"\n    return yolo_detect_and_crop(img)  # Directly using YOLO integration for precise cropping\n\ndef extract_roi(img):\n    \"\"\"Extracts Region of Interest using improved cervix segmentation.\"\"\"\n    cropped_img = cropCircle(img)\n    if cropped_img is None:\n        return None\n    imgLab = cv2.cvtColor(cropped_img, cv2.COLOR_RGB2LAB)\n    w, h = cropped_img.shape[:2]\n    Ra = np.zeros((w * h, 2))\n    for i in range(w):\n        for j in range(h):\n            R = math.sqrt((w / 2 - i) ** 2 + (h / 2 - j) ** 2)\n            Ra[i * h + j, 0] = R\n            Ra[i * h + j, 1] = min(imgLab[i][j][1], 150)\n    Ra[:, 0] /= max(Ra[:, 0])\n    Ra[:, 0] *= 1.0\n    Ra[:, 1] /= max(Ra[:, 1])\n\n    g = GaussianMixture(n_components=2, covariance_type='diag', random_state=0, init_params='kmeans')\n    g.fit(shuffle(Ra, random_state=0)[:1000])\n    labels = g.predict(Ra)\n    labels_2D = np.reshape(labels + 1, (w, h))\n    cervix_cluster = np.argmax(np.bincount(labels)) + 1\n\n    mask = (labels_2D == cervix_cluster).astype('uint8')\n    return mask * cropped_img\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T05:12:38.274840Z","iopub.execute_input":"2024-12-24T05:12:38.275215Z","iopub.status.idle":"2024-12-24T05:12:38.291206Z","shell.execute_reply.started":"2024-12-24T05:12:38.275185Z","shell.execute_reply":"2024-12-24T05:12:38.289915Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Implement YOLO stage for bounding box detection\ndef yolo_detect_and_crop(image_path):\n    \"\"\"Detects cervical regions using YOLO and crops the ROI.\"\"\"\n    yolo_model = yolov5.load('yolov5s')  # Load YOLOv5 model (or replace with custom-trained model)\n    results = yolo_model(image_path)\n    bbox = results.xyxy[0]  # Extract bounding boxes\n    if len(bbox) > 0:\n        bbox = bbox[0]  # Select the first bounding box\n        x_min, y_min, x_max, y_max = map(int, bbox[:4])\n        img = cv2.imread(image_path)\n        cropped_img = img[y_min:y_max, x_min:x_max]\n        return cropped_img\n    return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:08:55.301804Z","iopub.execute_input":"2024-12-24T04:08:55.302158Z","iopub.status.idle":"2024-12-24T04:08:55.307722Z","shell.execute_reply.started":"2024-12-24T04:08:55.302130Z","shell.execute_reply":"2024-12-24T04:08:55.306613Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Prepare dataset splits\n\nroot_dir = '../input/intel-mobileodt-cervical-cancer-screening'\ntrain_dir = os.path.join(root_dir,'train', 'train')\n\ntype1_dir = os.path.join(train_dir, 'Type_1')\ntype2_dir = os.path.join(train_dir, 'Type_2')\ntype3_dir = os.path.join(train_dir, 'Type_3')\n\ntrain_type1_files = glob.glob(type1_dir+'/*.jpg')\ntrain_type2_files = glob.glob(type2_dir+'/*.jpg')\ntrain_type3_files = glob.glob(type3_dir+'/*.jpg')\n\nadded_type1_files  =  glob.glob(os.path.join(root_dir, \"additional_Type_1_v2\", \"Type_1\")+'/*.jpg')\nadded_type2_files  =  glob.glob(os.path.join(root_dir, \"additional_Type_2_v2\", \"Type_2\")+'/*.jpg')\nadded_type3_files  =  glob.glob(os.path.join(root_dir, \"additional_Type_3_v2\", \"Type_3\")+'/*.jpg')\n\n\ntype1_files = train_type1_files + added_type1_files\ntype2_files = train_type2_files + added_type2_files\ntype3_files = train_type3_files + added_type3_files\n\n# create dataframe of file and labels\nfiles = {'filepath': type1_files + type2_files + type3_files,\n          'label': ['Type 1']* len(type1_files) + ['Type 2']* len(type2_files) + ['Type 3']* len(type3_files)}\n\nfiles_df = pd.DataFrame(files).sample(frac=1, random_state= 1).reset_index(drop=True)\n# describe the dataframe\nfiles_df.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:08:56.739540Z","iopub.execute_input":"2024-12-24T04:08:56.739963Z","iopub.status.idle":"2024-12-24T04:08:56.900744Z","shell.execute_reply.started":"2024-12-24T04:08:56.739930Z","shell.execute_reply":"2024-12-24T04:08:56.899878Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                                 filepath   label\ncount                                                8215    8215\nunique                                               8215       3\ntop     ../input/intel-mobileodt-cervical-cancer-scree...  Type 2\nfreq                                                    1    4348","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filepath</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>8215</td>\n      <td>8215</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>8215</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>../input/intel-mobileodt-cervical-cancer-scree...</td>\n      <td>Type 2</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>1</td>\n      <td>4348</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# check for duplicates\nlen(files_df[files_df.duplicated(subset=['filepath'])])\n\n# check for damaged files\nbad_files = []\nfor path in (files_df['filepath'].values):\n    try:\n        img = Image.open(path)\n    except:\n        index = files_df[files_df['filepath']==path].index.values[0]\n        bad_files.append(index)\nprint(len(bad_files))\n\n# drop the damaged files\nfiles_df.drop(bad_files, inplace=True)\n\n# check unique labels\nfiles_df['label'].unique()\n\n# get count of each type \ntype_count = pd.DataFrame(files_df['label'].value_counts()).rename(columns= {'label': 'Num_Values'})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:09:04.611948Z","iopub.execute_input":"2024-12-24T04:09:04.612345Z","iopub.status.idle":"2024-12-24T04:09:25.630309Z","shell.execute_reply.started":"2024-12-24T04:09:04.612314Z","shell.execute_reply":"2024-12-24T04:09:25.629241Z"}},"outputs":[{"name":"stdout","text":"3\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"#  split the data into train  and validation set\ntrain_df, eval_df = train_test_split(files_df, test_size= 0.2, stratify= files_df['label'], random_state= 1)\nval_df, test_df = train_test_split(eval_df, test_size= 0.5, stratify= eval_df['label'], random_state= 1)\nprint(len(train_df), len(val_df), len(test_df))\n\n# loads images from dataframe\ndef load_images(dataframe):\n    features = []\n    filepaths = dataframe['filepath'].values\n    labels = dataframe['label'].values\n    \n    for path in filepaths:\n        img = cv2.imread(path)\n        resized_img = cv2.resize(img, (224, 224))\n        features.append(np.array(resized_img))\n    return np.array(features), np.array(labels)\n# load training and evaluation data\ntrain_features, train_labels = load_images(train_df)\nval_features, val_labels = load_images(val_df)\ntest_features, test_labels = load_images(test_df)\n\n# normalize the features\nX_train = train_features/255\nX_val  = val_features/255\nX_test  = test_features/255\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:28:54.460092Z","iopub.execute_input":"2024-12-24T04:28:54.460495Z","iopub.status.idle":"2024-12-24T04:49:15.393358Z","shell.execute_reply.started":"2024-12-24T04:28:54.460460Z","shell.execute_reply":"2024-12-24T04:49:15.392221Z"}},"outputs":[{"name":"stdout","text":"6569 821 822\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\n# Fit label encoder\nle = LabelEncoder().fit(['Type 1', 'Type 2', 'Type 3'])\n# Convert labels to integer encoding first, then to one-hot encoding\ny_train = to_categorical(le.transform(train_labels), num_classes=3)\ny_val = to_categorical(le.transform(val_labels), num_classes=3)\ny_test = to_categorical(le.transform(test_labels), num_classes=3)\n\n# initialize image data generator for training and evaluation sets\ntrain_datagen = ImageDataGenerator(\n                                rotation_range = 40,\n                                zoom_range = 0.2,\n                                width_shift_range=0.2,\n                                height_shift_range=0.2,\n                                shear_range=0.2,\n                                horizontal_flip=True,\n                                vertical_flip = True)\n\neval_datagen = ImageDataGenerator()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:52:30.818439Z","iopub.execute_input":"2024-12-24T04:52:30.818857Z","iopub.status.idle":"2024-12-24T04:52:30.831912Z","shell.execute_reply.started":"2024-12-24T04:52:30.818823Z","shell.execute_reply":"2024-12-24T04:52:30.831070Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"BATCH_SIZE= 32\ntrain_gen = train_datagen.flow(X_train, y_train, batch_size= BATCH_SIZE)\nval_gen = eval_datagen.flow(X_val, y_val, batch_size= BATCH_SIZE)\ntest_gen = eval_datagen.flow(X_test, y_test, batch_size= BATCH_SIZE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:52:34.860091Z","iopub.execute_input":"2024-12-24T04:52:34.860415Z","iopub.status.idle":"2024-12-24T04:52:39.707979Z","shell.execute_reply.started":"2024-12-24T04:52:34.860389Z","shell.execute_reply":"2024-12-24T04:52:39.707149Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# def DSPP(input_tensor, l2_rate=1e-4):\n#     pool = AveragePooling2D(pool_size=(input_tensor.shape[1], input_tensor.shape[2]))(input_tensor)\n#     pool = Conv2D(128, 1, padding=\"same\", activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(l2_rate))(pool)\n#     pool = UpSampling2D(size=(input_tensor.shape[1] // pool.shape[1], input_tensor.shape[2] // pool.shape[2]), interpolation=\"bilinear\")(pool)\n\n#     dilated_1 = Conv2D(128, 3, dilation_rate=1, padding=\"same\", activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(l2_rate))(input_tensor)\n#     dilated_6 = Conv2D(128, 3, dilation_rate=6, padding=\"same\", activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(l2_rate))(input_tensor)\n#     dilated_12 = Conv2D(128, 3, dilation_rate=12, padding=\"same\", activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(l2_rate))(input_tensor)\n#     dilated_18 = Conv2D(128, 3, dilation_rate=18, padding=\"same\", activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(l2_rate))(input_tensor)\n\n#     x = concatenate([pool, dilated_1, dilated_6, dilated_12, dilated_18])\n#     return Conv2D(128, 1, padding=\"same\", activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(l2_rate))(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:52:42.468204Z","iopub.execute_input":"2024-12-24T04:52:42.468540Z","iopub.status.idle":"2024-12-24T04:52:42.476740Z","shell.execute_reply.started":"2024-12-24T04:52:42.468513Z","shell.execute_reply":"2024-12-24T04:52:42.475552Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Modified ResNet-50 model without DSPP\nbase_model = ResNet50(input_shape=(224, 224, 3), include_top=False, weights=\"imagenet\")\nfor layer in base_model.layers[:-10]:\n    layer.trainable = False\n\nx = base_model.output\nx = Flatten()(x)\nx = Dropout(0.3)(x)\nx = Dense(128, activation='relu')(x)\nx = Dense(3, activation='softmax')(x)\n\nmodel1 = Model(inputs=base_model.input, outputs=x)\nmodel1.compile(optimizer=Adam(learning_rate=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n\nmodel2 = Model(inputs=base_model.input, outputs=x)\nmodel2.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T05:13:27.934944Z","iopub.execute_input":"2024-12-24T05:13:27.935280Z","iopub.status.idle":"2024-12-24T05:13:30.162104Z","shell.execute_reply.started":"2024-12-24T05:13:27.935254Z","shell.execute_reply":"2024-12-24T05:13:30.160583Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\nfrom collections import Counter\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\n# Assume train_df is a pandas DataFrame with a 'label' column\n# Convert labels to a NumPy array (if not already)\nlabels = train_df['label'].values\n\n# Compute class weights\nweights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(labels),\n    y=labels\n)\n\n# Convert to a dictionary {class_index: weight}\nunique_classes = np.unique(labels)\nclass_weight_dict = dict(zip(unique_classes, weights))\n\n# Then pass it into model.fit(...)\nmodel1.fit(\n    train_gen,\n    validation_data=val_gen,\n    epochs=30,\n    callbacks=[\n        EarlyStopping(patience=10, restore_best_weights=True),\n        ReduceLROnPlateau(patience=5)\n    ],\n    class_weight=class_weight_dict\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T05:17:09.425212Z","iopub.execute_input":"2024-12-24T05:17:09.425575Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/30\n\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m792s\u001b[0m 4s/step - accuracy: 0.4408 - loss: 3.4077 - val_accuracy: 0.2948 - val_loss: 1.2271 - learning_rate: 0.0010\nEpoch 2/30\n\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m777s\u001b[0m 4s/step - accuracy: 0.4775 - loss: 1.0966 - val_accuracy: 0.5201 - val_loss: 1.0257 - learning_rate: 0.0010\nEpoch 3/30\n\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m795s\u001b[0m 4s/step - accuracy: 0.4890 - loss: 1.0545 - val_accuracy: 0.5286 - val_loss: 1.0210 - learning_rate: 0.0010\nEpoch 4/30\n\u001b[1m 17/206\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10:53\u001b[0m 3s/step - accuracy: 0.5057 - loss: 1.0703","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Training two models with different learning rates and datasets\nhistory1 = model1.fit(\n    train_gen,\n    validation_data=val_gen,\n    epochs=30,\n    callbacks=[EarlyStopping(patience=10, restore_best_weights=True), ReduceLROnPlateau(patience=5)],\n    class_weight=class_weight_dict\n)\n\nhistory2 = model2.fit(\n    train_gen,\n    validation_data=val_gen,\n    epochs=30,\n    callbacks=[EarlyStopping(patience=10, restore_best_weights=True), ReduceLROnPlateau(patience=5)],\n    class_weight=class_weight_dict\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:53:41.062048Z","iopub.execute_input":"2024-12-24T04:53:41.062390Z","iopub.status.idle":"2024-12-24T04:53:41.093117Z","shell.execute_reply.started":"2024-12-24T04:53:41.062361Z","shell.execute_reply":"2024-12-24T04:53:41.091850Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-5b9ac597b075>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_best_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NumpyArrayIterator' object has no attribute 'classes'"],"ename":"AttributeError","evalue":"'NumpyArrayIterator' object has no attribute 'classes'","output_type":"error"}],"execution_count":20},{"cell_type":"code","source":"# Ensembling predictions\ndef ensemble_predictions(model1, model2, test_gen):\n    predictions1 = model1.predict(test_gen)\n    predictions2 = model2.predict(test_gen)\n    return (predictions1 + predictions2) / 2\n\nensemble_preds = ensemble_predictions(model1, model2, test_data)\ny_true = test_gen.classes\ny_pred = np.argmax(ensemble_preds, axis=-1)\n\n# Evaluation\neval_metrics = model1.evaluate(test_data)  # Using model1 metrics as a reference\nprecision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\nprint(f\"Test Accuracy: {eval_metrics[1] * 100:.2f}%\")\nprint(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-Score: {f1:.2f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}